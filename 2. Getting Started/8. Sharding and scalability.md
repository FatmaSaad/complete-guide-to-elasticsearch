1

00:00:02,540  -->  00:00:06,760
A bit earlier you learned how a cluster consists
of one or more nodes.

2

00:00:07,500  -->  00:00:12,760
This is one of the ways in which Elasticsearch
can scale, both in regards to data storage

3

00:00:12,760  -->  00:00:13,940
and disk space.

4

00:00:14,880  -->  00:00:20,820
If we want to store 1 terabyte of data and
we only have a single node with 500 gigabytes

5

00:00:20,820  -->  00:00:23,880
of space, that's obviously not going to
work.

6

00:00:24,620  -->  00:00:30,339
However, if we add an additional node with
sufficient capacity, Elasticsearch can then

7

00:00:30,340  -->  00:00:35,720
store data on both nodes, meaning that the
cluster now has enough storage capacity.

8

00:00:36,420  -->  00:00:38,700
But how does that actually work?

9

00:00:39,360  -->  00:00:42,900
Elasticsearch does this by using something
called sharding.

10

00:00:43,780  -->  00:00:49,220
You might have heard of this term in the context
of databases, for instance, and the concept

11

00:00:49,220  -->  00:00:51,300
is the same, so let's take a look.

12

00:00:52,340  -->  00:00:57,920
Sharding is a way to divide an index into
separate pieces, where each piece is called

13

00:00:57,920  -->  00:00:58,660
a shard.

14

00:00:59,880  -->  00:01:05,100
Notice how sharding is done at the index level,
and not at the cluster or node level.

15

00:01:05,840  -->  00:01:10,740
That's because one index might contain a
billion documents, while another may only

16

00:01:10,740  -->  00:01:12,360
contain a few hundred.

17

00:01:13,000  -->  00:01:17,020
As you might imagine, this affects how the
indices are configured.

18

00:01:17,600  -->  00:01:23,360
The main reason for dividing an index into
multiple shards, is to be able to horizontally

19

00:01:23,360  -->  00:01:25,120
scale the data volume.

20

00:01:25,560  -->  00:01:29,060
Let me give you a simple example of how sharding
works.

21

00:01:30,080  -->  00:01:35,720
Suppose that we have two nodes each having
500 gigabytes of storage space available for

22

00:01:35,720  -->  00:01:36,720
Elasticsearch.

23

00:01:37,340  -->  00:01:43,640
We have a huge index taking up 600 gigabytes
of storage on its own, meaning that the whole

24

00:01:43,640  -->  00:01:45,920
index does not fit on either node.

25

00:01:46,720  -->  00:01:52,250
Therefore, running the index on a single shard
is not an option, because a shard needs to

26

00:01:52,250  -->  00:01:54,100
be placed on a single node.

27

00:01:54,800  -->  00:02:00,860
Instead, we can divide the index into two
shards, each requiring 300 gigabytes worth

28

00:02:00,860  -->  00:02:02,220
of disk space.

29

00:02:03,160  -->  00:02:09,080
Doing this, we can now store a shard on each
of the two nodes without running out of disk space.

30

00:02:10,320  -->  00:02:16,520
We could also have a higher number of shards
if we wanted to, such as four shards of 150

31

00:02:16,520  -->  00:02:17,800
gigabytes each.

32

00:02:18,720  -->  00:02:24,080
We still have space to spare, so we could
use that for other indices if we needed to.

33

00:02:25,300  -->  00:02:31,660
Just to be clear, a shard may be placed on
any node, so if an index has five shards,

34

00:02:31,670  -->  00:02:36,660
for instance, we don't need to spread these
out on five different nodes.

35

00:02:36,660  -->  00:02:38,900
We could, but we don't have to.

36

00:02:40,060  -->  00:02:45,480
Alright, hopefully you understand how sharding
makes it possible to scale the amount of documents

37

00:02:45,480  -->  00:02:46,600
we can store.

38

00:02:47,100  -->  00:02:52,320
Of course sharding goes hand in hand with
increasing the available disk space if necessary,

39

00:02:52,680  -->  00:02:55,520
potentially by adding more nodes to the cluster.

40

00:02:56,180  -->  00:03:01,200
Each shard is independent, and you can think
of a shard as being a fully functional index

41

00:03:01,200  -->  00:03:02,180
on its own.

42

00:03:02,940  -->  00:03:05,900
That's not 100% accurate, but close enough.

43

00:03:07,080  -->  00:03:10,560
Remember how Elasticsearch is built on top
of Apache Lucene?

44

00:03:11,200  -->  00:03:14,180
Each shard is actually a Lucene index.

45

00:03:14,780  -->  00:03:21,080
This means that an Elasticsearch index with
five shards, actually consists of five Lucene

46

00:03:21,080  -->  00:03:22,320
indices under the hood.

47

00:03:23,540  -->  00:03:29,420
While a shard does not have a predefined size
in terms of allocated disk space, there is

48

00:03:29,420  -->  00:03:35,020
a limit to the number of documents a shard
can store, being just over two billion documents.

49

00:03:36,460  -->  00:03:42,180
As I said a few moments ago, the main reason
for sharding an index, is to be able to scale

50

00:03:42,180  -->  00:03:45,860
its data volume, being the number of documents
it can store.

51

00:03:46,640  -->  00:03:51,760
By using sharding, you can store billions
of documents within a single index, which

52

00:03:51,760  -->  00:03:54,840
would usually not be feasible without sharding.

53

00:03:55,920  -->  00:04:01,460
Another common use case for sharding an index,
is to divide an index into smaller chunks

54

00:04:01,460  -->  00:04:03,720
that are easier to fit onto nodes.

55

00:04:04,610  -->  00:04:10,240
Yet another reason why sharding is useful,
is that it enables queries to be distributed

56

00:04:10,240  -->  00:04:13,260
and parallelized across an index' shards.

57

00:04:14,160  -->  00:04:20,460
What this means, is that a search query can
be run on multiple shards at the same time,

58

00:04:20,460  -->  00:04:22,720
increasing the performance and throughput.

59

00:04:23,490  -->  00:04:28,580
Remember that the shards may be stored on
different nodes, meaning that the hardware

60

00:04:28,580  -->  00:04:30,740
of multiple nodes may be utilized.

61

00:04:31,680  -->  00:04:36,900
If you are wondering how Elasticsearch searches
for data across multiple shards, then don't

62

00:04:36,900  -->  00:04:41,020
worry about that for now, as I will get back
to that later in the course when relevant.

63

00:04:42,340  -->  00:04:46,820
Let's head over to Kibana, where I have
prepared a query, which you have already seen

64

00:04:46,820  -->  00:04:47,820
before.

65

00:04:48,220  -->  00:04:51,660
Let's go ahead and run it to list the indices
within our cluster.

66

00:04:53,240  -->  00:05:00,960
Looking at the results, we can see a column
named "pri," being short for "primary shards."

67

00:05:01,620  -->  00:05:06,460
I haven't told you what a primary
shard is yet, but I will in the next lecture.

68

00:05:06,880  -->  00:05:13,100
Until then, just think that this column represents
the number of shards that a given index has.

69

00:05:14,100  -->  00:05:19,759
We can see that each of our indices are stored
on a single shard, being the default setting,

70

00:05:19,760  -->  00:05:22,600
but this can be configured when creating indices.

71

00:05:23,880  -->  00:05:30,440
Up until Elasticsearch version 7, the default
was actually for an index to have five shards.

72

00:05:31,000  -->  00:05:35,740
However, having five shards for very small
indices is very unnecessary.

73

00:05:36,440  -->  00:05:41,819
When people created lots of small indices
within small clusters, they ran into an issue

74

00:05:41,820  -->  00:05:46,120
of over-sharding, meaning that they had way
too many shards.

75

00:05:47,080  -->  00:05:53,259
At the same time, it was previously not possible
to change the number of shards once an index

76

00:05:53,260  -->  00:05:54,500
had been created.

77

00:05:54,840  -->  00:06:00,020
So what you had to do if you needed to increase
the number of shards, was to create a new

78

00:06:00,020  -->  00:06:03,740
index with more shards, and move over the
documents.

79

00:06:04,220  -->  00:06:08,880
This was not only inconvenient, but could
also be a time consuming process.

80

00:06:09,820  -->  00:06:15,800
To overcome these challenges, indices are
created with a single shard by default, since

81

00:06:15,820  -->  00:06:17,640
Elasticsearch version 7.

82

00:06:18,620  -->  00:06:23,280
For small to medium sized indices, this is
likely to be sufficient.

83

00:06:23,760  -->  00:06:28,820
If you do need to increase the number of shards,
there is a Split API for this.

84

00:06:29,760  -->  00:06:35,620
It still involves creating a new index, but
the process is much easier, as Elasticsearch

85

00:06:35,620  -->  00:06:37,780
handles the heavy lifting for us.

86

00:06:38,740  -->  00:06:44,660
To go the other way, i.e. to reduce the number
of shards, there is a Shrink API that does

87

00:06:44,660  -->  00:06:45,720
this for us.

88

00:06:47,000  -->  00:06:52,200
These are both topics for later in the course,
but for now I just want you to know that the

89

00:06:52,200  -->  00:06:58,000
default number of shards for an index is one,
and that this can be changed if need be.

90

00:06:58,960  -->  00:07:04,780
That being said, you should try to anticipate
how many documents an index will contain in

91

00:07:04,780  -->  00:07:08,260
the not too distant future when creating an
index.

92

00:07:08,840  -->  00:07:13,300
If you think that you will store millions
of documents within an index, you might want

93

00:07:13,300  -->  00:07:16,420
to consider adding a few shards at index creation.

94

00:07:17,480  -->  00:07:22,520
Adding the additional shards at the beginning
is just easier for you, and you are less likely

95

00:07:22,520  -->  00:07:25,120
to run into bottlenecks down the road.

96

00:07:25,760  -->  00:07:28,760
So how many shards should you choose, then?

97

00:07:29,340  -->  00:07:31,920
This is a question I get asked a lot.

98

00:07:32,880  -->  00:07:38,300
Unfortunately there is no correct answer to
this question, other than "it depends."

99

00:07:38,880  -->  00:07:44,500
I know, that's probably not the answer you
were hoping for... There is no definitive

100

00:07:44,509  -->  00:07:50,309
answer to this, because it depends on a number
of factors, such as the number of nodes within

101

00:07:50,309  -->  00:07:56,129
the cluster, the capacity of the nodes, the
number of indices and their sizes, the number

102

00:07:56,129  -->  00:07:59,260
of queries run against the indices, etc.

103

00:08:00,080  -->  00:08:05,740
Needless to say, there are a lot of variables
involved, so there is really no formula that

104

00:08:05,740  -->  00:08:06,680
you can use.

105

00:08:07,700  -->  00:08:13,419
However, as a rule of thumb, a decent place
to start, would be to choose five shards if

106

00:08:13,420  -->  00:08:17,100
you anticipate millions of documents to be
added to an index.

107

00:08:17,420  -->  00:08:22,600
Otherwise, you should be completely fine sticking
to the default of one shard and then increase

108

00:08:22,600  -->  00:08:24,060
the number if need be.

109

00:08:25,400  -->  00:08:31,740
To recap, sharding is a way to sub-divide
an index into smaller pieces, each being a shard.

110

00:08:32,900  -->  00:08:38,600
This serves two main purposes; enabling the
index to grow in size, and to improve the

111

00:08:38,600  -->  00:08:40,260
throughput of the index.

112

00:08:41,160  -->  00:08:47,480
The main reason is to scale data storage,
so the increased throughput is probably more

113

00:08:47,480  -->  00:08:48,120
of a bonus.

114

00:08:49,140  -->  00:08:55,300
By using sharding, you can store an index
taking up 700 gigabytes of disk space, even

115

00:08:55,300  -->  00:08:58,520
if you have no single node that can store
that amount of data.

116

00:08:59,500  -->  00:09:01,880
An index defaults to having one shard.

117

00:09:02,500  -->  00:09:05,580
For small to medium indices, that's usually
enough.

118

00:09:06,840  -->  00:09:11,500
For indices that you anticipate will store
lots of data, you might want to increase the

119

00:09:11,500  -->  00:09:13,720
number of shards at index creation.

120

00:09:14,380  -->  00:09:20,280
A decent number for most of these cases would
be five, but it depends on a number of factors.

121

00:09:21,020  -->  00:09:25,560
If in doubt, I recommend that you just go
with the default values and take it from there.
