1

00:00:02,590  -->  00:00:06,521
We just talked about sharding and replication.

2

00:00:06,521  -->  00:00:11,800
Sharding enables Elasticsearch to scale the
number of documents an index can store by

3

00:00:11,800  -->  00:00:15,230
splitting it into smaller pieces.

4

00:00:15,230  -->  00:00:21,100
Naturally this is not going to help us forever
if we don’t add more nodes to our cluster,

5

00:00:21,100  -->  00:00:26,660
as we would otherwise run out of disk space
no matter how many shards we create.

6

00:00:26,660  -->  00:00:32,599
Likewise, replication only kicks in for clusters
consisting of more than one node.

7

00:00:32,599  -->  00:00:37,780
So instead of just talking about it, let’s
actually start up another node and see how

8

00:00:37,780  -->  00:00:40,366
our cluster behaves.

9

00:00:40,366  -->  00:00:45,541
In fact, just for the sake of it, we’ll
be spinning up two more nodes.

10

00:00:45,541  -->  00:00:50,445
This lecture is optional, so you are welcome
to skip it if it doesn’t interest you.

11

00:00:51,433  -->  00:00:56,739
If you created a cloud deployment instead
of a local one, adding additional nodes is

12

00:00:56,739  -->  00:01:00,252
handled within the cloud interface.

13

00:01:00,252  -->  00:01:05,820
This lecture therefore covers how to do this
on a local setup, but I do encourage you to

14

00:01:05,820  -->  00:01:09,709
stick around even if you are using a cloud
based deployment.

15

00:01:10,768  -->  00:01:16,522
Anyway, let’s begin by looking at the current
state of our cluster with the Cluster Health API.

16

00:01:19,010  -->  00:01:23,609
Unsurprisingly, our cluster consists of a
single node.

17

00:01:23,609  -->  00:01:28,840
The status is “yellow” because the pages
index that we created in the previous lecture

18

00:01:28,840  -->  00:01:31,880
has one unassigned replica shard.

19

00:01:31,880  -->  00:01:34,757
We can see that if we run the second query.

20

00:01:35,762  -->  00:01:41,749
The system indices will have one replica shard
created automatically when we add an additional node,

21

00:01:41,749  -->  00:01:46,392
so these indices are not the reason
for the “yellow” status.

22

00:01:46,392  -->  00:01:51,399
Don’t worry if you don’t see exactly the
same indices, because these system indices

23

00:01:51,399  -->  00:01:54,481
change quite frequently between releases.

24

00:01:55,989  -->  00:02:01,396
Alright, time to start another node and add
it to our single node cluster.

25

00:02:01,396  -->  00:02:07,712
First, make sure that you have the archive of 
Elasticsearch from when we set things up earlier.

26

00:02:07,712  -->  00:02:12,313
Then extract it for each additional node that
you want to add to your cluster.

27

00:02:12,313  -->  00:02:17,013
That’s twice in my case, because I want
to add two more nodes.

28

00:02:17,013  -->  00:02:22,143
It’s important that you do not make a copy
of your existing node’s directory, 

29

00:02:22,143  -->  00:02:25,390
because it contains data used by that node.

30

00:02:25,390  -->  00:02:31,159
So be sure to extract the archive to get a
clean start for each additional node.

31

00:02:31,159  -->  00:02:36,860
I have done this in advance and renamed the
directories to something meaningful.

32

00:02:36,860  -->  00:02:41,640
Before starting up the two nodes, I want to
quickly configure their names to make them

33

00:02:41,640  -->  00:02:45,500
easier to tell apart when inspecting the cluster.

34

00:02:45,500  -->  00:02:52,567
To do that, let’s open up a file named elasticsearch.yml,
located within the "config" directory.

35

00:02:56,642  -->  00:03:03,513
Near the top of the file, we can find a setting called 
"node.name," which is commented out by default.

36

00:03:03,513  -->  00:03:05,539
Let’s give it a meaningful value.

37

00:03:11,643  -->  00:03:15,565
That’s all, so I will just save the file
and do the same for the other node.

38

00:03:27,663  -->  00:03:28,696
That’s it.

39

00:03:28,696  -->  00:03:31,109
Let’s spin up some nodes.

40

00:03:31,109  -->  00:03:37,190
To add a node, we need an enrollment token,
exactly like when we set up Kibana.

41

00:03:37,190  -->  00:03:42,319
Note that this is only needed when starting
up a new node for the first time; once it

42

00:03:42,319  -->  00:03:47,912
has joined the cluster, we can start it up
as normal without specifying a token.

43

00:03:47,912  -->  00:03:53,181
With the enrollment token, Elasticsearch takes
care of a bunch of things for us, meaning

44

00:03:53,181  -->  00:03:57,674
that we don’t have to configure things within
configuration files.

45

00:03:57,674  -->  00:04:02,690
The Kibana enrollment token was generated
for us automatically, but this time we need

46

00:04:02,690  -->  00:04:04,780
to run a script.

47

00:04:04,780  -->  00:04:10,842
To do that, open up a terminal with the working
directory set to the running node’s root directory.

48

00:04:13,477  -->  00:04:20,768
The script we need is named "elasticsearch-create-enrollment-token"
and is located within the "bin" directory.

49

00:04:27,410  -->  00:04:34,148
To tell the script which kind of enrollment token 
we want to generate, we need to provide a scope.

50

00:04:34,148  -->  00:04:36,860
We will set it to "node" this time.

51

00:04:36,860  -->  00:04:41,986
If we wanted to generate a new Kibana enrollment
token, we would simply set it to "kibana."

52

00:04:44,033  -->  00:04:46,690
That’s it — let’s run the command.

53

00:04:46,690  -->  00:04:50,723
Make sure that your existing node is running
or the command will fail.

54

00:04:53,397  -->  00:04:56,619
Alright, we now have an enrollment token.

55

00:04:56,619  -->  00:04:59,163
Let’s copy it and start up a second node.

56

00:05:03,041  -->  00:05:09,770
I have a terminal opened up with the working
directory set to the second node’s root directory.

57

00:05:09,770  -->  00:05:15,060
All we need to do is to run the elasticsearch
script within the bin directory, only this

58

00:05:15,060  -->  00:05:18,337
time we need to provide an enrollment token.

59

00:05:18,337  -->  00:05:19,681
Let’s type that out.

60

00:05:26,108  -->  00:05:27,893
Alright, let’s run the command.

61

00:05:28,970  -->  00:05:34,280
The node will take a moment or two to start
up, so I will just do a bit of time traveling.

62

00:05:35,788  -->  00:05:37,689
Alright, I’m back!

63

00:05:37,689  -->  00:05:42,740
Our second node has now been started up and
has been added to our cluster.

64

00:05:42,740  -->  00:05:47,620
If you check the terminal in which the first
node is running, you should see some output

65

00:05:47,620  -->  00:05:49,688
that proves this.

66

00:05:49,688  -->  00:05:55,357
I’m just going to go back to Kibana and inspect 
our cluster so you can see that everything worked.

67

00:05:58,559  -->  00:06:03,720
If we check our cluster’s health now, we
can see that the cluster now consists of two

68

00:06:03,720  -->  00:06:07,990
nodes, and that the health has transitioned
to “green.”

69

00:06:07,990  -->  00:06:14,638
That’s because the replica shard for the "pages" 
index has now been assigned to our second node.

70

00:06:14,638  -->  00:06:20,915
This effectively means that we can lose one
of our nodes without losing any data.

71

00:06:20,915  -->  00:06:24,624
Let’s check how the shards have been distributed
to verify this.

72

00:06:28,340  -->  00:06:32,951
Indeed we can see that the replica shard has
been assigned to our second node.

73

00:06:33,849  -->  00:06:38,363
We can also see that the system indices now
have a replica shard each.

74

00:06:39,063  -->  00:06:43,961
That’s because they were all configured
with the "auto_expand_replicas setting," meaning

75

00:06:43,961  -->  00:06:48,471
that a replica shard is added when an additional
node is added to the cluster.

76

00:06:49,764  -->  00:06:55,169
Also notice how the primary and replica shards
are stored on different nodes, exactly as

77

00:06:55,169  -->  00:06:57,991
we talked about in the previous lecture.

78

00:06:57,991  -->  00:07:02,931
That’s because a primary and replica shard
are never stored on the same node.

79

00:07:02,931  -->  00:07:08,449
If that were the case, we could lose an entire
replication group if a node had a hardware

80

00:07:08,449  -->  00:07:09,700
failure, for instance.

81

00:07:10,867  -->  00:07:15,821
Alright, so that’s how to add additional
nodes to your cluster.

82

00:07:15,821  -->  00:07:20,842
Note that this approach is only meant for
development purposes; 

83

00:07:20,842  -->  00:07:26,460
setting up a multi-node cluster in a production environment
requires some configuration.

84

00:07:27,584  -->  00:07:31,449
I am going to start up a third node using
the same approach.

85

00:07:31,449  -->  00:07:35,349
If you are following along, I do want to mention
something first.

86

00:07:36,139  -->  00:07:43,201
If your cluster consists of three nodes, you are 
are required to run at least two of them moving forward.

87

00:07:43,201  -->  00:07:48,162
In other words, you won’t be able to run
your cluster using a single node.

88

00:07:48,162  -->  00:07:53,270
Without getting into details, this has to
do with how Elasticsearch elects a so-called

89

00:07:53,270  -->  00:08:00,319
master node, which it cannot do for a three-node
cluster where only one node is available.

90

00:08:00,319  -->  00:08:04,520
The point is that if you want to just run
a single node while you work your way through

91

00:08:04,520  -->  00:08:08,009
this course, don’t start up a third node.

92

00:08:08,009  -->  00:08:13,418
Alternatively you can of course try it out
and just set up a fresh cluster afterwards.

93

00:08:14,244  -->  00:08:18,186
Anyway, I just wanted to mention that so you
are not caught by surprise.

94

00:08:20,215  -->  00:08:24,864
Alright, let’s create another enrollment
token, this time for a third node.

95

00:08:29,441  -->  00:08:31,487
Let’s copy it and start up the node.

96

00:08:44,359  -->  00:08:47,339
Alright, I’ll see you when the node has
started up.

97

00:08:50,750  -->  00:08:53,277
There we go, the node is now ready.

98

00:08:53,277  -->  00:08:56,010
Let’s inspect the cluster once again.

99

00:08:56,010  -->  00:08:59,242
In particular, I want to take a look at our shards.

100

00:09:02,761  -->  00:09:08,824
Notice how our shards have now been distributed
across our three nodes automatically.

101

00:09:08,824  -->  00:09:14,070
Even though our indices fit perfectly well
on two nodes, Elasticsearch automatically

102

00:09:14,070  -->  00:09:17,310
distributes them evenly across the available nodes.

103

00:09:18,100  -->  00:09:23,460
That’s just a primitive way of scaling things,
because we don’t want our third node to sit idle.

104

00:09:25,017  -->  00:09:30,445
As you can see, Elasticsearch takes care of
a lot of things for us automatically, 

105

00:09:30,445  -->  00:09:32,852
making our jobs much easier.

106

00:09:32,852  -->  00:09:34,090
Pretty cool, right?

107

00:09:35,275  -->  00:09:38,670
Let’s see what happens if we kill one of
our nodes.

108

00:09:40,340  -->  00:09:45,780
Hitting the CTRL + C [control and C] keys
simultaneously gracefully shuts down the node.

109

00:09:45,780  -->  00:09:51,170
That will do just fine, but let’s simulate
that something went wrong and forcefully shut

110

00:09:51,170  -->  00:09:54,820
down the terminal tab in which the node is
running.

111

00:09:54,820  -->  00:09:59,440
This will kill the process abruptly, but the
result is really the same.

112

00:10:02,833  -->  00:10:06,053
Alright, our second node is now gone.

113

00:10:06,053  -->  00:10:09,296
Let’s head over to the terminal where our
first node is running.

114

00:10:10,840  -->  00:10:15,411
The reason I am looking here is that this
is our master node.

115

00:10:15,411  -->  00:10:18,070
I will get back to what this means in the
next lecture.

116

00:10:19,129  -->  00:10:25,560
Within the output we can see that a node left
the cluster because the connection to it was lost.

117

00:10:25,560  -->  00:10:31,020
We can also see some output saying that delayed
shards have been scheduled to be rerouted

118

00:10:31,020  -->  00:10:33,090
in about 60 seconds.

119

00:10:34,149  -->  00:10:39,700
Let’s head over to Kibana to inspect our
shards and I will explain what this is all about.

120

00:10:42,506  -->  00:10:46,523
Notice how the shards that were stored on
the leaving node are unassigned.

121

00:10:47,475  -->  00:10:51,350
These shards have been marked to be moved
to different nodes.

122

00:10:51,350  -->  00:10:53,429
This happens after about a minute.

123

00:10:54,650  -->  00:10:59,520
The reason for that delay is that this can
be an expensive maneuver depending on the

124

00:10:59,520  -->  00:11:04,640
sizes of the shards, and so we don’t want
it to happen immediately in the case of a

125

00:11:04,640  -->  00:11:07,221
brief networking failure, for instance.

126

00:11:08,370  -->  00:11:13,740
I won’t get into the details of what happens
when a node leaves the cluster, but I have

127

00:11:13,740  -->  00:11:18,823
attached a link to the documentation in case
you want to do some reading on the subject.

128

00:11:20,187  -->  00:11:26,089
Anyway, I will just head back to the terminal and 
wait for the delayed shard reallocation to happen.

129

00:11:29,740  -->  00:11:35,093
We can now see that the cluster’s health
has transitioned from “yellow” to “green.”

130

00:11:35,093  -->  00:11:37,363
Let’s take a look at the shards once again.

131

00:11:39,050  -->  00:11:44,760
The unassigned shards have now been reallocated
to other nodes, and the shards have been balanced

132

00:11:44,760  -->  00:11:46,643
between the available nodes.

133

00:11:47,522  -->  00:11:53,000
This means that our cluster is now fully functional
and still protected from data loss in the

134

00:11:53,000  -->  00:11:55,244
case where another node is lost.

135

00:11:56,770  -->  00:12:02,660
If you are following along with me, you can try 
to start the node that we shut down back up.

136

00:12:02,660  -->  00:12:07,820
You should then see that the shards are balanced
across the three nodes once again, just as

137

00:12:07,820  -->  00:12:11,022
if the node never left the cluster in the
first place.

138

00:12:11,974  -->  00:12:17,250
Anyway, that’s how to add additional nodes
to your development cluster and how Elasticsearch

139

00:12:17,250  -->  00:12:20,507
handles nodes joining and leaving the cluster.
