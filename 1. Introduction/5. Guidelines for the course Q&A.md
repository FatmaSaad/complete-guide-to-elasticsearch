1

00:00:01,599  -->  00:00:05,980
Now that you know what Elasticsearch can be
used for and understand what the Elastic Stack

2

00:00:05,980  -->  00:00:11,220
is, let’s take a look at a couple of architectures
and how Elasticsearch is commonly used.

3

00:00:11,220  -->  00:00:14,340
Let’s start out simple and go from there.

4

00:00:14,340  -->  00:00:18,100
Suppose that we have an ecommerce application
running on a web server.

5

00:00:18,100  -->  00:00:24,180
The data is stored within a database, such
as the product categories and the products themselves.

6

00:00:24,180  -->  00:00:28,960
So when a product page is requested, the web
application looks up the product within the

7

00:00:28,960  -->  00:00:33,809
database, renders the page, and sends it back
to the visitor’s browser.

8

00:00:33,809  -->  00:00:38,399
Now we want to improve the search functionality
on the website because so far, it has just

9

00:00:38,399  -->  00:00:40,859
been using the database for this.

10

00:00:40,859  -->  00:00:45,010
But as you might know, that’s not what databases
are really good at.

11

00:00:45,010  -->  00:00:49,469
So after looking around online, we decided
to go with Elasticsearch, because that seems

12

00:00:49,469  -->  00:00:51,840
to be the best tool for the job.

13

00:00:51,840  -->  00:00:57,000
That’s an excellent choice, but how do we
integrate it with our current architecture?

14

00:00:57,000  -->  00:01:01,460
The easiest way, is to communicate with Elasticsearch
from our application.

15

00:01:01,460  -->  00:01:06,939
So when someone enters a search query on our
website, a request is sent to the web application,

16

00:01:06,940  -->  00:01:10,440
which then sends a search query to Elasticsearch.

17

00:01:10,440  -->  00:01:15,100
This can be done with a plain HTTP request,
but typically you will use one of the client

18

00:01:15,100  -->  00:01:19,789
libraries that are provided for all popular
programming languages.

19

00:01:19,789  -->  00:01:24,380
When the application receives a response,
it can process it and send the results back

20

00:01:24,380  -->  00:01:26,280
to the browser.

21

00:01:26,280  -->  00:01:32,680
But how do we get data into Elasticsearch
in the first place, and how do we keep it updated?

22

00:01:32,680  -->  00:01:37,640
Whenever a new product is added or updated,
we should add or update the product within

23

00:01:37,640  -->  00:01:42,020
Elasticsearch too, apart from within the database.

24

00:01:42,020  -->  00:01:45,300
So essentially we’ll duplicate some of our
data.

25

00:01:45,300  -->  00:01:51,200
That might sound like a bad idea, but I will
get back to why this is actually the best approach.

26

00:01:51,200  -->  00:01:55,720
Okay, that’s great if we are launching our
website with Elasticsearch from the beginning,

27

00:01:55,729  -->  00:02:00,689
but what if we are adding it to an existing
application that already has data, and potentially

28

00:02:00,689  -->  00:02:02,120
lots of it?

29

00:02:02,120  -->  00:02:05,340
How do we get that data into Elasticsearch
then?

30

00:02:05,340  -->  00:02:08,000
Well, that takes a bit more work.

31

00:02:08,009  -->  00:02:13,250
You could write a script that does it all
in one go if you don’t have that much data.

32

00:02:13,250  -->  00:02:17,080
If you have more, you would need to paginate
- or scroll through - your data.

33

00:02:17,080  -->  00:02:21,640
There are some open source projects that can
help with this, but not all of them are actively

34

00:02:21,640  -->  00:02:26,700
maintained, and usually it’s a fairly simple
task to write a script that does this.

35

00:02:26,700  -->  00:02:31,160
Then from the moment the data has been imported,
your application will keep it up to date whenever

36

00:02:31,160  -->  00:02:33,160
something changes.

37

00:02:33,160  -->  00:02:38,420
That’s probably the simplest usage of Elasticsearch
that you will come across, so let’s expand

38

00:02:38,430  -->  00:02:40,030
a bit on that architecture.

39

00:02:40,030  -->  00:02:44,780
Our boss has told us that he wants a dashboard
where he can see the number of orders per

40

00:02:44,780  -->  00:02:47,480
week, the revenue, etc.

41

00:02:47,480  -->  00:02:53,190
We could build our own interface, but we would
save a lot of time by using Kibana, so that’s

42

00:02:53,190  -->  00:02:55,450
what we have decided to do.

43

00:02:55,450  -->  00:03:00,720
As you know, Kibana is just a web interface
that interacts with Elasticsearch, so we don’t

44

00:03:00,720  -->  00:03:03,320
need to add any data to it or anything.

45

00:03:03,320  -->  00:03:07,230
We just need to run it somewhere, on a machine
of our choice.

46

00:03:07,230  -->  00:03:11,260
To keep things simple, let’s just assume
that we dedicate a machine - or a virtual

47

00:03:11,260  -->  00:03:14,160
machine - to run Kibana on.

48

00:03:14,160  -->  00:03:19,560
So all we need to do, is to run Kibana on
a machine and configure it to connect to Elasticsearch.

49

00:03:19,560  -->  00:03:21,340
That’s it.

50

00:03:21,350  -->  00:03:25,710
As time passes, the business continues to
grow, and we are really starting to see a

51

00:03:25,710  -->  00:03:27,860
lot of traffic on the website.

52

00:03:27,860  -->  00:03:33,700
Our server is starting to sweat a bit, and
we want to make sure that we can handle future growth.

53

00:03:33,700  -->  00:03:38,340
A part of this is to handle spikes in traffic
and knowing when it’s time to add another

54

00:03:38,340  -->  00:03:39,750
web server.

55

00:03:39,750  -->  00:03:44,400
To help us with this, we have decided to make
use of Metricbeat to monitor system level

56

00:03:44,400  -->  00:03:49,160
metrics, such as CPU usage and memory usage.

57

00:03:49,160  -->  00:03:54,510
We can also instruct Metricbeat to monitor
our web server process if we want to.

58

00:03:54,510  -->  00:04:00,560
So we start up Metricbeat on the web server,
but how does the data end up in Elasticsearch?

59

00:04:00,560  -->  00:04:05,490
As you might remember from the previous lecture,
Beats supports sending data to both Logstash

60

00:04:05,490  -->  00:04:07,760
and Elasticsearch.

61

00:04:07,760  -->  00:04:11,500
In terms of simplicity, we will do the latter.

62

00:04:11,500  -->  00:04:16,580
What we will actually be doing, is to have
Metricbeat send data to a so-called Elasticsearch

63

00:04:16,580  -->  00:04:18,280
ingest node.

64

00:04:18,280  -->  00:04:24,380
The details of this is not important for now,
so don’t worry about what an ingest node is for now.

65

00:04:24,380  -->  00:04:28,880
The point is that we can send data to Elasticsearch,
and we can perform simple transformations

66

00:04:28,880  -->  00:04:31,910
on our data in case we need to.

67

00:04:31,910  -->  00:04:38,200
Now that system metrics are being stored within
Elasticsearch, we can visualize the data within Kibana.

68

00:04:38,200  -->  00:04:42,960
What’s really cool, is that we can actually
instruct Metricbeat to configure a dashboard

69

00:04:42,960  -->  00:04:44,620
within Kibana.

70

00:04:44,620  -->  00:04:49,460
That’s just a default dashboard that displays
data that you are probably interested in.

71

00:04:49,460  -->  00:04:53,920
You can of course tweak it to your needs,
so consider it a template.

72

00:04:53,920  -->  00:04:58,460
But how does Metricbeat actually do this when
it doesn’t communicate directly with Kibana,

73

00:04:58,460  -->  00:04:59,880
you might wonder?

74

00:04:59,880  -->  00:05:05,200
Well, Kibana stores its configuration within
Elasticsearch, so no matter where your Kibana

75

00:05:05,200  -->  00:05:11,120
instance runs - or if you start up a fresh
instance - your settings will be applied automatically.

76

00:05:11,120  -->  00:05:15,960
A bit later in the course, you will see exactly
where the configuration is stored.

77

00:05:15,960  -->  00:05:18,070
Okay, so far so good.

78

00:05:18,070  -->  00:05:22,790
We can now monitor how our server is performing,
and we can set up alerts within Kibana if

79

00:05:22,790  -->  00:05:27,650
we want to be notified if the CPU or memory
usage reaches some threshold.

80

00:05:27,650  -->  00:05:31,900
When that happens, we should be ready to add
an additional web server.

81

00:05:31,900  -->  00:05:37,100
As the business continues to grow, so does
the development team and the code base.

82

00:05:37,100  -->  00:05:41,640
Apart from monitoring system level information
on our web server, we want to monitor two

83

00:05:41,640  -->  00:05:46,560
more things; the access and error logs from
our web server.

84

00:05:46,560  -->  00:05:51,020
While we do make use of Google Analytics on
the website, the access log can provide us

85

00:05:51,020  -->  00:05:57,180
with some useful information, such as how
long it took the web server to process each request.

86

00:05:57,180  -->  00:06:02,840
This enables us to monitor the response times
and aggregate the requests per endpoint.

87

00:06:02,840  -->  00:06:08,280
By doing this, we can easily identify if some
bad code is deployed, causing the response

88

00:06:08,280  -->  00:06:11,360
time to spike for a specific endpoint.

89

00:06:11,360  -->  00:06:16,520
Not only can this cause a bad user experience,
but it can also put additional stress on our

90

00:06:16,520  -->  00:06:19,800
system, so we want to detect if that happens.

91

00:06:19,810  -->  00:06:24,730
Since the business has grown, so has the number
of features that need to be implemented.

92

00:06:24,730  -->  00:06:29,180
The development team is bigger now, and as
a result we are able to push out more code

93

00:06:29,180  -->  00:06:30,790
to production.

94

00:06:30,790  -->  00:06:35,120
Of course that also increases the risk of
introducing bugs, so we want to stay on top

95

00:06:35,120  -->  00:06:39,620
of that apart from the testing that we do
before deploying new code.

96

00:06:39,620  -->  00:06:43,830
The web server’s error log can help us with
that, and optionally any application log that

97

00:06:43,830  -->  00:06:45,680
we might have.

98

00:06:45,680  -->  00:06:48,440
Filebeat is a perfect tool for this job!

99

00:06:48,440  -->  00:06:52,780
All we need to do, is to start up Filebeat,
and we will have our logs processed and stored

100

00:06:52,780  -->  00:06:56,680
within Elasticsearch, ready for analysis within
Kibana.

101

00:06:56,680  -->  00:06:58,660
Pretty easy, right?

102

00:06:58,669  -->  00:07:03,200
Filebeat has built-in modules for this, which
takes care of the heavy lifting in terms of

103

00:07:03,200  -->  00:07:08,770
parsing the logs, handling multi-line events
such as stack traces, etc., so for simple

104

00:07:08,770  -->  00:07:13,400
use cases, we don’t need any configuration
to get this to work.

105

00:07:13,400  -->  00:07:16,919
Okay, so let’s fast forward 6 months.

106

00:07:16,919  -->  00:07:21,911
In the meantime, we have added two more web
servers to handle more traffic, so our architecture

107

00:07:21,920  -->  00:07:23,900
now looks like this.

108

00:07:23,900  -->  00:07:28,380
We are storing more kinds of data within Elasticsearch,
including events.

109

00:07:28,380  -->  00:07:32,650
An example could be when a product is added
to the cart, because we then want to do some

110

00:07:32,650  -->  00:07:35,160
analysis within Kibana.

111

00:07:35,160  -->  00:07:39,850
So far, we have been using Elasticsearch’s
ingest nodes, where we could do very simple

112

00:07:39,850  -->  00:07:41,770
data transformation.

113

00:07:41,770  -->  00:07:46,640
But now we need to do more advanced processing
of our events, such as data enrichment or

114

00:07:46,640  -->  00:07:48,810
whatever the case might be.

115

00:07:48,810  -->  00:07:54,590
We could do this within our web application,
but this has a few disadvantages; first of

116

00:07:54,590  -->  00:07:59,840
all, our business logic code will be cluttered
with event processing, and this might increase

117

00:07:59,840  -->  00:08:03,400
response time unless we do this asynchronously.

118

00:08:03,400  -->  00:08:07,979
Nevertheless, it leads to more code, which
is not directly related to actually adding

119

00:08:07,980  -->  00:08:10,760
a product to the cart, for instance.

120

00:08:10,760  -->  00:08:14,720
Secondly, event processing will happen in
multiple places.

121

00:08:14,720  -->  00:08:19,100
It’s not that big of a problem that it happens
on more than one web server, because they

122

00:08:19,100  -->  00:08:21,830
should all be running the same version of
the code.

123

00:08:21,830  -->  00:08:26,430
But we might have a backoffice server where
administrative tasks are performed, such as

124

00:08:26,430  -->  00:08:28,210
managing products.

125

00:08:28,210  -->  00:08:32,440
Or perhaps we even want to migrate to a microservice
architecture.

126

00:08:32,440  -->  00:08:36,999
Whatever the case might be, we can quickly
end up in a situation where events are processed

127

00:08:37,000  -->  00:08:41,460
in many different places, making things harder
to maintain.

128

00:08:41,460  -->  00:08:47,060
Wouldn’t it be better if we could centralize
the event processing and have it all done in one place?

129

00:08:47,060  -->  00:08:51,800
We can accomplish that with Logstash, which
also gives us more flexibility in terms of

130

00:08:51,800  -->  00:08:56,600
processing our events than Elasticsearch’s
ingest nodes do.

131

00:08:56,600  -->  00:08:59,720
So let’s go ahead and add Logstash to the
architecture.

132

00:08:59,720  -->  00:09:04,520
We’ll be sending events from our web servers
to Logstash over HTTP.

133

00:09:04,520  -->  00:09:11,080
Logstash will then process the events however
we instruct it to, and send them off to Elasticsearch.

134

00:09:11,080  -->  00:09:15,980
This way, we keep event processing out of
our web application; all they need to do,

135

00:09:15,980  -->  00:09:21,960
is to send the initial data to Logstash, and
there we will be able to do the rest of the processing.

136

00:09:21,960  -->  00:09:25,580
But what about the data from Metricbeat and
Filebeat?

137

00:09:25,589  -->  00:09:30,490
As for Metricbeat, chances are that we don’t
need to do any custom processing of the data,

138

00:09:30,490  -->  00:09:34,740
but that might be the case for Filebeat if
we are using custom logging formats, or if

139

00:09:34,740  -->  00:09:37,779
we want to do some additional processing.

140

00:09:37,779  -->  00:09:42,740
If that’s the case - or if we just prefer
to centralize things - we can definitely send

141

00:09:42,740  -->  00:09:45,260
the data to Logstash for processing.

142

00:09:45,260  -->  00:09:50,240
Otherwise it’s also okay to send data directly
to Elasticsearch if we don’t mind that data

143

00:09:50,240  -->  00:09:53,459
is being added from multiple places.

144

00:09:53,459  -->  00:09:56,279
So what about the rest of the data, then?

145

00:09:56,280  -->  00:10:01,180
For instance when a new product is added or
a product is modified or deleted.

146

00:10:01,180  -->  00:10:04,380
You could leave everything as is, and have
your web application

147

00:10:04,380  -->  00:10:07,400
update the data in Elasticsearch directly.

148

00:10:07,400  -->  00:10:11,840
That’s the easiest approach for sure, but
it is also more error prone, as code

149

00:10:11,840  -->  00:10:15,540
errors in the application might break event
processing.

150

00:10:15,540  -->  00:10:20,559
On the short term, this would be acceptable
while migrating everything to Logstash events,

151

00:10:20,560  -->  00:10:24,240
keeping everything centralized within Logstash
pipelines.

152

00:10:24,240  -->  00:10:29,420
Ideally, our web application would only be
querying Elasticsearch for data, not modifying

153

00:10:29,420  -->  00:10:34,529
any data, so in a perfect world, read-only
access should be sufficient.

154

00:10:34,529  -->  00:10:38,230
Of course that might not always be the case
in the real world, but it’s a good goal

155

00:10:38,230  -->  00:10:40,820
to have nevertheless.

156

00:10:40,820  -->  00:10:47,240
Alright, that was an example of a simple architecture
evolving over time to become slightly more advanced.

157

00:10:47,240  -->  00:10:51,459
That’s a pretty typical example of how you
will use Elasticsearch and the Elastic Stack

158

00:10:51,460  -->  00:10:56,680
in general, although there are of course variations
and other use cases as well.
