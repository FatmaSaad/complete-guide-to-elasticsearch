1

00:00:02,280  -->  00:00:07,741
In the previous lecture you learned the very
basics of analysis in Elasticsearch, and in

2

00:00:07,741  -->  00:00:09,800
particular the “standard” analyzer.

3

00:00:10,440  -->  00:00:14,200
You saw an example of how a string is analyzed
by default.

4

00:00:15,080  -->  00:00:20,600
There is actually a very convenient API that
you can use to test how Elasticsearch analyzes

5

00:00:20,600  -->  00:00:21,640
a given string.

6

00:00:22,140  -->  00:00:27,960
This is useful in a couple of scenarios, such
as for building custom analyzers, and — as

7

00:00:27,960  -->  00:00:34,000
in this case — experimenting to better understand
what happens as part of the analysis process.

8

00:00:34,960  -->  00:00:38,420
Let’s write a query to test the “standard”
analyzer.

9

00:00:38,880  -->  00:00:43,780
So the endpoint is “_analyze,” and the
HTTP verb should be POST.

10

00:00:47,380  -->  00:00:52,160
We supply the text that we want to be analyzed
within a parameter named “text.”

11

00:00:57,960  -->  00:01:00,740
As 
for the text, I will just paste in a really

12

00:01:00,740  -->  00:01:04,500
lame joke, so my apologies for the poor sense
of humor! ;-)

13

00:01:06,740  -->  00:01:08,540
I know, it’s not even funny.

14

00:01:09,360  -->  00:01:11,980
Anyway, it will work just fine for us.

15

00:01:12,460  -->  00:01:17,720
If we were to run the query now, the text
would be analyzed with the “standard” analyzer.

16

00:01:18,700  -->  00:01:24,300
That’s actually what we want, but let’s
just specify this explicitly with the “analyzer” parameter.

17

00:01:32,120  -->  00:01:37,540
In place of “standard,” we can write the
name of other built-in analyzers or custom ones.

18

00:01:38,060  -->  00:01:40,560
Let’s run the query and inspect the result.

19

00:01:42,260  -->  00:01:45,900
The result contains the tokens that were emitted
by the analyzer.

20

00:01:46,300  -->  00:01:52,180
Apart from the token itself, each token also
contains some metadata, most of which I briefly

21

00:01:52,180  -->  00:01:53,980
mentioned in the previous lecture.

22

00:01:54,560  -->  00:01:59,460
Among this metadata is a type, which will
usually be alphanumeric.

23

00:02:00,320  -->  00:02:06,040
In our case, the text included a number, and
so the first token is of the numeric type.

24

00:02:06,840  -->  00:02:10,280
We can also see the start and end offsets
for each token.

25

00:02:11,260  -->  00:02:16,260
These are the character offsets for where the token appeared in the text before it was analyzed.

26

00:02:17,640  -->  00:02:22,140
Alright, so we can see that our text has been
tokenized at word boundaries.

27

00:02:23,300  -->  00:02:28,620
It might look like the tokenization was done
based on whitespaces, but that is not the

28

00:02:28,620  -->  00:02:32,480
case, because the algorithm is a bit more
advanced than that.

29

00:02:33,520  -->  00:02:39,420
For instance, if we were to remove the whitespace
between the three periods and the word “ducks,”

30

00:02:39,420  -->  00:02:43,280
we would still end up with two tokens being
“third” and “ducks.”

31

00:02:44,740  -->  00:02:50,260
The “standard” tokenizer also takes care
of removing most symbols, such as commas,

32

00:02:50,260  -->  00:02:52,480
exclamation marks, periods, etc.

33

00:02:53,020  -->  00:02:57,040
That’s because these provide no value when
performing full-text searches.

34

00:02:57,700  -->  00:03:03,480
It also trims whitespaces, so the triple whitespace
that’s part of our text is just ignored.

35

00:03:05,360  -->  00:03:10,099
Looking at the last token, we can see that
the “lowercase” token filter has also

36

00:03:10,100  -->  00:03:14,480
been at work, since the word “ducks” is
no longer in uppercase letters.

37

00:03:15,620  -->  00:03:20,200
There is really nothing new here, as all of
this is just a practical way of showing you

38

00:03:20,200  -->  00:03:22,260
what you learned in the previous lecture.

39

00:03:23,200  -->  00:03:28,440
Instead of specifying an analyzer, we can
also specify the parts making up an analyzer,

40

00:03:28,440  -->  00:03:32,440
being character filters, a tokenizer, and
token filters.

41

00:03:33,440  -->  00:03:38,580
Let’s replicate the “standard” analyzer
by specifying its components explicitly, just

42

00:03:38,580  -->  00:03:40,500
to show you that the result is the same.

43

00:03:41,420  -->  00:03:45,720
I will just make a copy of the existing query
and remove the “analyzer” parameter.

44

00:03:49,860  -->  00:03:56,260
First, let’s add a parameter named “char_filter,”
of course being where we define any character filters.

45

00:04:01,720  -->  00:04:06,060
Since the “standard” analyzer doesn’t
make use of any character filters, I will

46

00:04:06,060  -->  00:04:07,580
just leave the array empty.

47

00:04:08,060  -->  00:04:12,940
I could have left out the parameter entirely
as the behavior would be the same, but let’s

48

00:04:12,940  -->  00:04:14,200
just be explicit.

49

00:04:15,320  -->  00:04:19,580
Next, let’s add the “standard” tokenizer
with the “tokenizer” parameter.

50

00:04:26,020  -->  00:04:30,280
The last thing we need to do, is to specify
the “lowercase” token filter.

51

00:04:30,680  -->  00:04:33,440
We do that with a parameter named “filter.”

52

00:04:39,920  -->  00:04:40,980
That’s it!

53

00:04:41,000  -->  00:04:44,020
That’s essentially all the “standard”
analyzer does.

54

00:04:49,060  -->  00:04:53,180
Scrolling through the results, we can see
that the result is the same as before.

55

00:04:54,440  -->  00:04:59,660
When we get to building custom analyzers,
keep this API in mind, because it is super

56

00:04:59,660  -->  00:05:03,580
useful for testing things out before applying
them to documents.

57

00:05:04,180  -->  00:05:05,700
That’s it for this lecture.

58

00:05:05,980  -->  00:05:11,000
In the next one, we will take a look at what actually happens with the tokens that an analyzer yields.
