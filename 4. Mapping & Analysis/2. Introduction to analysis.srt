1

00:00:02,600  -->  00:00:05,700
The first topic that we will cover is analysis.

2

00:00:06,040  -->  00:00:10,360
You might also see this topic being referred
to as text analysis.

3

00:00:10,940  -->  00:00:15,260
That’s because the concept is really only
applicable to text values.

4

00:00:15,780  -->  00:00:19,920
When we index a text value, it goes through
an analysis process.

5

00:00:20,460  -->  00:00:23,920
This process involves a number of things that
I will get back to.

6

00:00:24,720  -->  00:00:30,000
The purpose of it is to store the values in
a data structure that is efficient for searching.

7

00:00:31,120  -->  00:00:36,659
When we retrieved documents in the previous
section, you saw how the values that we indexed

8

00:00:36,660  -->  00:00:39,280
were returned under an “_source” key.

9

00:00:40,220  -->  00:00:45,660
This object contains the exact values that
we specified when indexing a document.

10

00:00:46,540  -->  00:00:53,260
However, those are not the values that Elasticsearch
uses internally when figuring out which documents

11

00:00:53,260  -->  00:00:54,720
match a search query.

12

00:00:55,260  -->  00:01:00,600
That’s because a long product description,
for instance, cannot be searched efficiently

13

00:01:00,600  -->  00:01:02,420
without processing it first.

14

00:01:03,080  -->  00:01:09,380
I will tell you the specifics of how Elasticsearch
stores data soon, but for now the point is

15

00:01:09,380  -->  00:01:12,260
that text is processed before being stored.

16

00:01:13,780  -->  00:01:19,580
When a text value is indexed, a so-called
analyzer is used to process the text.

17

00:01:20,320  -->  00:01:23,120
In other words, the value is analyzed.

18

00:01:23,940  -->  00:01:32,200
An analyzer consists of three building blocks;
character filters, a tokenizer, and token filters.

19

00:01:33,000  -->  00:01:38,600
The result of analyzing text values is then
stored in a searchable data structure.

20

00:01:39,400  -->  00:01:45,020
Let’s go over the parts of an analyzer one
by one, beginning with character filters.

21

00:01:45,980  -->  00:01:52,440
A character filter receives the original text
and may transform it by adding, removing,

22

00:01:52,440  -->  00:01:53,940
or changing characters.

23

00:01:54,700  -->  00:02:00,079
An analyzer may have zero or more character
filters, and they are applied in the order

24

00:02:00,080  -->  00:02:01,740
in which they are specified.

25

00:02:02,440  -->  00:02:09,100
An example could be to remove HTML elements
and convert HTML entities by using a character

26

00:02:09,100  -->  00:02:12,280
filter named “html_strip.”

27

00:02:13,280  -->  00:02:20,000
An analyzer must contain exactly one tokenizer,
which is responsible for tokenizing the text.

28

00:02:20,620  -->  00:02:25,640
By “tokenizing,” I am referring to the
process of splitting the text into tokens.

29

00:02:26,340  -->  00:02:32,720
As part of that process, characters may be
removed, such as punctuation, exclamation

30

00:02:32,720  -->  00:02:33,980
marks, etc.

31

00:02:35,260  -->  00:02:40,720
An example of that could be to split a sentence
into words by splitting the string whenever

32

00:02:40,720  -->  00:02:42,620
a whitespace is encountered.

33

00:02:43,580  -->  00:02:47,880
The input string is therefore tokenized into
a number of tokens.

34

00:02:48,420  -->  00:02:54,540
Although you don’t see it in this example,
the tokenizer also records the character offsets

35

00:02:54,540  -->  00:02:57,180
for each token in the original string.

36

00:02:58,220  -->  00:03:03,100
You will learn why this is the case later
in the course, so I chose to keep the example

37

00:03:03,100  -->  00:03:05,080
as simple as possible for now.

38

00:03:06,120  -->  00:03:09,020
Next, we have token filters.

39

00:03:09,340  -->  00:03:15,720
These receive the tokens that the tokenizer
produced as input and they may add, remove,

40

00:03:15,720  -->  00:03:17,200
or modify tokens.

41

00:03:17,800  -->  00:03:23,720
As with character filters, an analyzer may
contain zero or more token filters, and they

42

00:03:23,720  -->  00:03:26,380
are applied in the order in which they are
specified.

43

00:03:27,450  -->  00:03:33,480
The simplest possible example of a token filter
is probably the “lowercase” filter, which

44

00:03:33,480  -->  00:03:35,540
lowercases all letters.

45

00:03:36,580  -->  00:03:42,300
Elasticsearch ships with a number of built-in
character filters, tokenizers, and token filters.

46

00:03:43,240  -->  00:03:49,840
As you can probably imagine, it is possible to mix and match these together to form custom analyzers.

47

00:03:50,540  -->  00:03:54,660
We will get back to that later in this section,
but let’s keep things simple for now.

48

00:03:55,720  -->  00:04:01,280
Let’s walk through an example of what happens by default when Elasticsearch encounters a text value.

49

00:04:02,620  -->  00:04:08,580
No character filter is used by default, so
the text is passed on to the tokenizer as is.

50

00:04:09,720  -->  00:04:15,380
The tokenizer splits the text into tokens
according to the Unicode Segmentation algorithm.

51

00:04:16,420  -->  00:04:21,959
Its implementation is a bit technical, but
essentially it breaks sentences into words

52

00:04:21,960  -->  00:04:24,540
by whitespace, hyphens, and such.

53

00:04:25,460  -->  00:04:33,020
In the process, it also throws away punctuation
such as commas, periods, exclamation marks, etc.

54

00:04:33,680  -->  00:04:39,480
It’s a bit more complicated than this, but
it pretty much behaves the way you would expect.

55

00:04:40,300  -->  00:04:44,200
The tokens are then passed on to a token filter
named “lowercase.”

56

00:04:44,800  -->  00:04:50,420
This token filter does what you would expect;
it lowercases all letters for the tokens.

57

00:04:51,840  -->  00:04:55,660
What you just saw is the behavior of the “standard”
analyzer.

58

00:04:56,080  -->  00:05:01,100
This analyzer is used for all “text” fields
unless configured otherwise.

59

00:05:01,940  -->  00:05:06,900
There are a couple of analyzers available
besides the “standard” analyzer, but that’s

60

00:05:06,900  -->  00:05:08,780
the one you will typically use.

61

00:05:09,980  -->  00:05:13,820
It’s also possible to build your own one
like I mentioned a moment ago.

62

00:05:14,540  -->  00:05:19,620
I will give you an overview of the most important
analyzers, character filters, tokenizers,

63

00:05:19,620  -->  00:05:22,400
and token filters a bit later in this section.

64

00:05:23,240  -->  00:05:29,160
All you need to know for now is what analyzers
do and how the “standard” analyzer behaves.
