You should now have a good understanding of how analyzers work and when they are used.

Elasticsearch comes with a number of built-in analyzers.

These are simply pre-configured combinations of character filters, token filters, and a tokenizer.

They are available for your convenience, so there is nothing magical about them; you can  make custom analyzers that do the same thing if you want to.

Let’s quickly go through the most important ones.

First, we have the "standard" analyzer, which we have already talked about.

It splits text into terms at word boundaries, and also removes punctuation in the process.

That’s done with the "standard" tokenizer.

Apart from that, it also lowercases letters with the "lowercase" token filter.

It actually contains the "stop" token filter as well, for removing stop words.

This filter is disabled by default, but can be enabled through configuration, which you  will see in a moment.

2022-09-19_01-51

The "simple" analyzer is quite similar to the "standard" analyzer.

The difference is that it splits the input text whenever it encounters anything other  than a letter, where the "standard" analyzer splits by word.

This analyzer also lowercases terms, but this is done by the "lowercase" tokenizer and  not a token filter.

That’s pretty unusual because a tokenizer is not really supposed to do that.

This is a performance hack to avoid passing through the input twice.

The "whitespace" analyzer simply splits the input text whenever it encounters a whitespace.

Unlike the other analyzers, it does not lowercase letters.

The "keyword" analyzer is a no-op analyzer, meaning that it leaves the input text intact  and simply outputs it as a single term.

Sounds familiar?

 This analyzer is actually used for fields of the "keyword" data type by default.

Remember how these fields are used for matching exact values?

 Behind the scenes, the "keyword" analyzer is used, and the exact input values are stored  within inverted indices.

The "pattern" analyzer lets you define a regular expression to match token separators.

This essentially means that you should match whatever should cause the input text to be  split into tokens.

This can be anything you want, really, so this analyzer provides a lot of flexibility.

The default pattern matches all non-word characters as **(\W+)**.

Note that the analyzer lowercases letters by default, but this can be disabled.

Last but not least, there are a number of language specific analyzers.

Let’s head over to the documentation for a moment.

This page shows a list of the built-in analyzers, most of which we just covered.

Let’s click the link to the (documentation)[https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html] for the language analyzers.

2022-09-19_03-02

At the top of the page, we can see a list of the available analyzers, covering most languages.

Let’s click the "english" one and see what it does.

This takes us to a query snippet showing how the analyzer could be implemented as a custom analyzer.

2022-09-19_03-06

That’s not how you would use the analyzer, unless you want to change things.

This just shows us the pieces the analyzer consists of so that we can understand what  it does.

2022-09-19_03-07

As we can see, it uses the "standard" tokenizer and a number of token filters.

The first one is a so-called possessive stemmer, which removes possession from words.

Possession typically refers to ‘s, as in "Peter’s dog".

It then lowercases all letters as most analyzers do and also removes English stop words.

The "keyword_marker" filter is a way to exclude words from stemming, but it is not  enabled by default, so let’s ignore that one.

Finally the analyzer stems words for the English language.

While this example is specific to the English language, this is more or less what most of  the analyzers look like.

Some of the analyzers do handle some language specific quirks, though.

As you can see, the language analyzers are just standard implementations that handle  basic things.

They are often a good starting point if you don’t need anything super advanced.

Sometimes you might want to expand the configuration though.

An example could be to handle synonyms, which is a topic that is covered later in the course.

2022-09-19_03-08

Those were the most interesting built-in analyzers.

Let’s now look at how we can use them within field mappings.

That’s actually extremely easy, because all we need to do is to specify the name of  an analyzer for a mapping parameter named "analyzer".

The following query shows how the "english" language analyzer can be specified for a "description" field.

This analyzer is used both at index and search time.

Pretty easy, right?



2022-09-19_03-09

While all of the built-in analyzers can be used in this way, many of them can also be  configured to adjust their behavior.

For instance, the "standard" analyzer can easily be configured to remove stop words,  which it doesn’t do by default.

Instead of having to implement a custom analyzer from scratch, we can configure the built-in analyzer.

We are actually creating a custom analyzer by extending the "standard" analyzer,  as specified within the "type" parameter.

In this example, I have given it a name of "remove_english_stop_words," but that’s  just an arbitrary name.

The structure of the query is a bit different compared to what you have seen so far, but  don’t worry about that for now; I will cover that in detail in the next lecture.

For now, let’s focus on the innermost object.

Besides specifying the type of analyzer, we simply add any parameters to this object.

So which ones are available, then?

 

We can check that within the documentation for each analyzer.

Let’s head back to the browser and go to the documentation for the ("standard")[https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html] analyzer.

If we scroll down a bit, we can see which parameters the analyzer supports.

In the example you saw a moment ago, I used the "stopwords" parameter, which we also see here.

2022-09-19_03-13

Okay, so now that we have created a custom analyzer, how do we use it?

2022-09-19_03-08

You probably won’t be surprised that we use it in the exact same way as you saw earlier;  by simply specifying its name.

That’s it for built-in analyzers!  Now it’s time to create a custom analyzer.

