# Using the Analyze API

In the previous lecture you learned the very basics of analysis in Elasticsearch, and in particular the "standard" analyzer.

You saw an example of how a string is analyzed by default.

There is actually a very convenient API that you can use to test how Elasticsearch analyzes a given string.

This is useful in a couple of scenarios, such as for building custom analyzers, and — as in this case — experimenting to better understand what happens as part of the analysis process.

Let’s write a query to test the "standard" analyzer.
```
POST /_analyze
{
"text" : "2 guys walk into a bar, but the third.. DUCKS!",
"analyzer" : "standard"
}
```
So the endpoint is "_analyze," and the HTTP verb should be POST.

We supply the text that we want to be analyzed within a parameter named "text".

As for the text, I will just paste in a really lame joke, so my apologies for the poor sense of humor! ;-)

I know, it’s not even funny.

Anyway, it will work just fine for us.

If we were to run the query now, the text would be analyzed with the "standard" analyzer.

That’s actually what we want, but let’s just specify this explicitly with the "analyzer" parameter.

In place of "standard," we can write the name of other built-in analyzers or custom ones.

Let’s run the query and inspect the result.

```
{
  "tokens" : [
    {
      "token" : "2",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<NUM>",
      "position" : 0
    },
    {
      "token" : "guys",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "walk",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "into",
      "start_offset" : 12,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "a",
      "start_offset" : 17,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "bar",
      "start_offset" : 19,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "but",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 28,
      "end_offset" : 31,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "third",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "ducks",
      "start_offset" : 40,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 9
    }
  ]
}

```

The result contains the tokens that were emitted by the analyzer.

Apart from the token itself, each token also contains some metadata, most of which I briefly mentioned in the previous lecture.

Among this metadata is a type, which will usually be alphanumeric.

In our case, the text included a number, and so the first token is of the numeric type.

We can also see the start and end offsets for each token.

These are the character offsets for where the token appeared in the text before it was analyzed.

Alright, so we can see that our text has been tokenized at word boundaries.

It might look like the tokenization was done based on whitespaces, but that is not the case, because the algorithm is a bit more advanced than that.

For instance, if we were to remove the whitespace between the three periods and the word "ducks," we would still end up with two tokens being "third" and "ducks".

The "standard" tokenizer also takes care of removing most symbols, such as commas, exclamation marks, periods, etc.

That’s because these provide no value when performing full-text searches.

It also trims whitespaces, so the triple whitespace that’s part of our text is just ignored.

Looking at the last token, we can see that the "lowercase" token filter has also been at work, since the word "ducks" is no longer in uppercase letters.

There is really nothing new here, as all of this is just a practical way of showing you what you learned in the previous lecture.

Instead of specifying an analyzer, we can also specify the parts making up an analyzer, being character filters, a tokenizer, and token filters.

Let’s replicate the "standard" analyzer by specifying its components explicitly, just to show you that the result is the same.

I will just make a copy of the existing query and remove the "analyzer" parameter.

```
POST /_analyze
{
"text" : "2 guys walk into a bar, but the third.. DUCKS!",
"char_filter": [],
"tokenizer": "standard",
"filter": ["lowercase"]
}
```
First, let’s add a parameter named "char_filter," of course being where we define any character filters.

Since the "standard" analyzer doesn’t make use of any character filters, I will just leave the array empty.

I could have left out the parameter entirely as the behavior would be the same, but let’s just be explicit.

Next, let’s add the "standard" tokenizer with the "tokenizer" parameter.

The last thing we need to do, is to specify the "lowercase" token filter.

We do that with a parameter named "filter".

That’s it!

That’s essentially all the "standard" analyzer does.

```
{
  "tokens" : [
    {
      "token" : "2",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<NUM>",
      "position" : 0
    },
    {
      "token" : "guys",
      "start_offset" : 2,
      "end_offset" : 6,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "walk",
      "start_offset" : 7,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "into",
      "start_offset" : 12,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "a",
      "start_offset" : 17,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "bar",
      "start_offset" : 19,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "but",
      "start_offset" : 24,
      "end_offset" : 27,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 28,
      "end_offset" : 31,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "third",
      "start_offset" : 32,
      "end_offset" : 37,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "ducks",
      "start_offset" : 40,
      "end_offset" : 45,
      "type" : "<ALPHANUM>",
      "position" : 9
    }
  ]
}
```
Scrolling through the results, we can see that the result is the same as before.

When we get to building custom analyzers, keep this API in mind, because it is super useful for testing things out before applying them to documents.

That’s it for this lecture.

In the next one, we will take a look at what actually happens with the tokens that an analyzer yields.

